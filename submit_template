#!/bin/bash

#SBATCH --time=12:00:00
#SBATCH --job-name=enceladu_plume
#SBATCH -o out.%x.%j  # STDOUT
#SBATCH -e err.%x.%j  # STDERR
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=32

source /etc/profile.d/modules.sh

#module load intel/18.1
#module load openmpi/3.1.2_intel-18.1

module load openmpi/3.1.2
module load pnetcdf/1.11.1_ompi-3.1.2

# change the working directory
cd $SLURM_SUBMIT_DIR
echo Working directory is $SLURM_SUBMIT_DIR

# write job information
echo Running on hosts $SLURM_NODELIST
echo Running on hosts $SLURM_JOB_NODELIST
echo Running job $SLURM_JOB_NAME
echo Running on $SLURM_NNODES nodes
echo Running on $SLURM_NPROCS processors
echo Time submitted: `date`

NODES=$SLURM_NNODES
NP=$SLURM_NPROCS
PPN=$SLURM_NTASKS_PER_NODE

# MPI info
echo MPI Used: `which mpirun`
echo This job has allocated $SLURM_NPROCS cpus on $SLURM_JOB_NUMNODES nodes

mpirun --map-by node -n $NP ./vortex.ex -i hpc0610d.inp
#mpirun --map-by node -n $NP ./vortex.ex -r vortex-0609b1.rst time/tlim=1600.E5